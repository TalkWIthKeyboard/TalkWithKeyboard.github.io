<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Mongodb CDC链路中的有序性 | GRYFFONDOR</title>
  <meta name="author" content="TalkWithKeyboard">
  
  <meta name="description" content="在我们的业务当中有这样一个场景，用户每收到一笔转账以后，就会通过推送服务给用户发送一个通知，而这些通知之间需要保证先后顺序（即帐单的产生时序）。假设我们有一个订单数据结构Bill：
1234567&amp;#123;	&#34;_id&#34;: ObjectId	&#34;fromUserId&#34;: String,	&#34;toUser">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Mongodb CDC链路中的有序性">
  <meta property="og:site_name" content="GRYFFONDOR">

  
    <meta property="og:image" content>
  

  
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

</head>
</html>
 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">GRYFFONDOR</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> Mongodb CDC链路中的有序性</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>在我们的业务当中有这样一个场景，用户每收到一笔转账以后，就会通过推送服务给用户发送一个通知，而这些通知之间需要保证先后顺序（即帐单的产生时序）。假设我们有一个订单数据结构<code>Bill</code>：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">"_id"</span>: ObjectId</span><br><span class="line">	<span class="string">"fromUserId"</span>: String,</span><br><span class="line">	<span class="attr">"toUserId"</span>: String,</span><br><span class="line">	<span class="attr">"amount"</span>: Double,</span><br><span class="line">	<span class="attr">"createdAt"</span>: Long</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>fromUserId</code>是转账用户的id，<code>toUserId</code>是收款用户的id，<code>amount</code>是转账金额，<code>createdAt</code>是记录创建的日期。</p>
<p>系统中使用安装了<code>debezium connector for mongodb</code>插件的<code>kafka connect</code>来将mongodb oplog收集到kafka中，下游使用<code>flink streaming task</code>来进行消费并触发推送服务。整个过程中数据会流过多个分布式系统，如何保证在流经这些系统以后还能保证记录的产生顺序就是今天要讨论的问题。</p>
<h2 id="Kafka-connect-amp-amp-Kafka"><a href="#Kafka-connect-amp-amp-Kafka" class="headerlink" title="Kafka connect &amp;&amp; Kafka"></a>Kafka connect &amp;&amp; Kafka</h2><p>所有的<code>Bill oplog</code>都会被发送到同一个<code>Kafka topic</code>中，所以只需要保证在发往Kafka的过程当中使用<code>toUserId</code>作为 Partition Key即可。但是由于使用了开源组件来帮助我们完成了收集oplog的任务，所以需要保证<code>debezium connector for mongodb</code>能如我们的愿。</p>
<p>首先需要知道的是<code>Kafka connect</code>默认情况下使用的分区策略是<code>org.apache.kafka.clients.producer.DefaultPartitioner</code>，其中<code>partition</code>方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">    <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">    <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">            <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">    AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">        counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">        AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">        <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">            counter = currentCounter;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果key是空的，则以轮训的方式来分配<code>partition</code>，否则则使用一种<code>murmur2</code>的HASH算法来分配<code>partition</code>。所以我们只需要将message key设置为<code>toUserId</code>即可达成我们的目的。但是 <a href="https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#mongodb-change-events-key" target="_blank" rel="noopener">Debezium Connector for MongoDB :: Change event’s key</a> 中写道，message key只能是<code>_id</code>，而<code>toUserId</code>不具有唯一性，所以不能作为<code>_id</code>；如果加上时间戳或者其他的字段保证了唯一性，又失去了要将相同<code>toUserId</code>放在一个分区的语义，所以这条路基本是走不通的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The MongoDB connector does not make any explicit determination of the topic partitions for events. Instead, it allows Kafka to determine the partition based upon the key. You can change Kafka’s partitioning logic by defining in the Kafka Connect worker configuration the name of the Partitioner implementation.</span><br><span class="line">Be aware that Kafka only maintains total order for events written to a single topic partition. Partitioning the events by key does mean that all events with the same key will always go to the same partition, ensuring that all events for a specific document are always totally ordered.</span><br></pre></td></tr></table></figure>

<p>继续寻找其他的变通方法，<a href="https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#partitions" target="_blank" rel="noopener">Debezium Connector for MongoDB :: Partitions</a> 文档中写到可以通过设置 Kafka connect worker 的<code>Partitioner</code>设置来指定分区策略。也就是 <a href="http://kafka.apache.org/documentation.html#producerconfigs" target="_blank" rel="noopener">Apache Kafka</a> 文档中提到的<code>partitioner.class</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">partitioner.class: Partitioner class that implements the org.apache.kafka.clients.producer.Partitioner interface.</span><br><span class="line">Type: classDefault: org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br></pre></td></tr></table></figure>

<p>即需要自己通过继承<code>Partitioner</code>接口来实现自己的分区策略，要实现<code>partition方法</code>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>实现了之后将包含该类的JAR包放入Kafka connect能扫描的路径下，再在<code>connect-standalone.properties</code>或者<code>connect-distributed.properties</code>配置文件中进行全局的设定。可以参见下面两个链接👇：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/55188508/custom-partition-assignment-in-kafka-jdbc-connector" target="_blank" rel="noopener">java - Custom partition assignment in Kafka JDBC connector - Stack Overflow</a></li>
<li><a href="https://stackoverflow.com/questions/44810221/setting-partition-strategy-in-a-kafka-connector" target="_blank" rel="noopener">java - Setting Partition Strategy in a Kafka Connector - Stack Overflow</a></li>
</ul>
<p>那么试想一下我们的方案是不是就可以实现为，将<code>toUserId</code>和时间戳拼接为唯一性的<code>_id</code>，在partition函数中将<code>toUserId</code>提取出来并以此作为partition key进行分区以实现在kafka中保证顺序的目的。但是这样的实现方案也存在问题，比如这个Kafka connect 仅来为这个任务服务，因为这种分区策略并不具有通用性，如果有多个类似的需求则要部署多份Kafka connect。同时也存在一些好处，例如拼接的<code>_id</code>是可以直接作为Mongodb的shard key来对该场景的上游进行横向扩展的。可惜的是debezium的实现并不能保证shard cluster传入kafka时保证事件的发生顺序，虽然能将拼接的<code>_id</code>作为shard key，但是由于这种架构并没有能力保证顺序性，所以这种扩展也是无效的，参见👇：<br><a href="https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#mongodb-sharded-cluster" target="_blank" rel="noopener">Debezium Connector for MongoDB :: MongoDB sharded cluster</a></p>
<p>综上在该架构中如果想在Kafka层保证事件的有序性是非常困难，并且很不经济实惠。</p>
<h2 id="Flink-streaming"><a href="#Flink-streaming" class="headerlink" title="Flink streaming"></a>Flink streaming</h2><p>通过上文的分析，我们不得不接受一个事实，<code>Flink</code>消费到的是乱序数据。但是因为其特性，能较为方便的对乱序数据进行处理。<br>在debezium收集到的oplog中，包含两个时间戳，一个是在Value payload中的<code>ts_ms</code>，表示的是debezium收集该oplog时的系统时间；另外一个是在Value payload中的<code>source.ts_ms</code>，表示的是mongodb产生oplog的时间。而我们的实体类中也写入了Create bill的时间<code>createdAt</code>，由于只关心<code>Insert</code>事件，所以在Value payload的<code>after.createdAt</code>中能获取Bill创建的真实时间。在使用过程中一般以第二个时间或者第三个时间为准，在没有采用shard cluster的mongodb中，我认为第二个时间的先后次序应该与第三个时间的先后次序相同；采用了shard cluster的mongodb中，应该以第三个时间为准。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">"payload": &#123;</span><br><span class="line">      "after": ...,</span><br><span class="line">      "patch": null,</span><br><span class="line">      "source": &#123;</span><br><span class="line">        "version": "1.0.3.Final",</span><br><span class="line">        "connector": "mongodb",</span><br><span class="line">        "name": "cdc_test",</span><br><span class="line">        "ts_ms": 1558965508000,</span><br><span class="line">        "snapshot": true,</span><br><span class="line">        "db": "inventory",</span><br><span class="line">        "rs": "rs0",</span><br><span class="line">        "collection": "bill",</span><br><span class="line">        "ord": 31,</span><br><span class="line">        "h": 1546547425148721999</span><br><span class="line">      &#125;,</span><br><span class="line">      "op": "r",</span><br><span class="line">      "ts_ms": 1558965515240</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>在Flink的时间概念中也有三种时间：</p>
<ul>
<li>事件时间：独立事件在产生它的设备上发生的事件，通常在进入Flink之前就已经嵌入到事件中。</li>
<li>接入时间：数据进入Flink的时间，取决于Source Operator所在主机的系统时钟。</li>
<li>处理时间：数据在操作算子计算过程中获取到的所在主机时间。</li>
</ul>
<p>显然在该场景下应该选择<code>Bill</code>的创建时间作为Flink的事件时间：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">env.addSource(<span class="type">Utils</span>.providesAvroKafkaConsumer(kafkaConfig))</span><br><span class="line">    .map(recordTuple =&gt; <span class="type">AvroMongoOplog</span>.newInstance(recordTuple._1.asInstanceOf[<span class="type">Record</span>], recordTuple._2.asInstanceOf[<span class="type">Record</span>]))</span><br><span class="line">    .filter(mongoOplog =&gt; mongoOplog.getOpType.eq(<span class="type">MongoOpType</span>.<span class="type">Insert</span>))</span><br><span class="line">    .assignAscendingTimestamps(mongoOplog =&gt; mongoOplog.getDocument.getLong(<span class="string">"createdAt"</span>))</span><br><span class="line">	  .keyBy(mongoOplog =&gt; mongoOplog.getDocument.get(<span class="string">"toUserId"</span>))</span><br><span class="line">    .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">    .process(...)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>连接Kafka，使用实现的<code>KeyedAvroDeserializationSchema</code>进行反序列化</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient</span><br><span class="line"><span class="keyword">import</span> org.apache.avro.generic.GenericRecord</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord</span><br><span class="line"></span><br><span class="line"><span class="meta">@SerialVersionUID</span>(<span class="number">1584533572114L</span>)</span><br><span class="line">class KeyedAvroDeserializationSchema extends KafkaDeserializationSchema[(GenericRecord, GenericRecord)] &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> keyDeserializer: ConfluentRegistryAvroDeserializationSchema[GenericRecord] = _</span><br><span class="line">    <span class="keyword">var</span> valueDeserializer: ConfluentRegistryAvroDeserializationSchema[GenericRecord] = _</span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">this</span><span class="params">(topic: String, schemaRegistry: String)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>()</span><br><span class="line">        val keySubject = topic + <span class="string">"-key"</span></span><br><span class="line">        val valueSubject = topic + <span class="string">"-value"</span></span><br><span class="line">        val schemaRegistryClient = <span class="keyword">new</span> CachedSchemaRegistryClient(schemaRegistry, <span class="number">1000</span>)</span><br><span class="line">        val keySchema = schemaRegistryClient.getByID(schemaRegistryClient.getLatestSchemaMetadata(keySubject).getId)</span><br><span class="line">        val valueSchema = schemaRegistryClient.getByID(schemaRegistryClient.getLatestSchemaMetadata(valueSubject).getId)</span><br><span class="line">        keyDeserializer = ConfluentRegistryAvroDeserializationSchema.forGeneric(keySchema, schemaRegistry)</span><br><span class="line">        valueDeserializer = ConfluentRegistryAvroDeserializationSchema.forGeneric(valueSchema, schemaRegistry)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">override def <span class="title">isEndOfStream</span><span class="params">(t: (GenericRecord, GenericRecord)</span>): Boolean </span>= <span class="keyword">false</span></span><br><span class="line"></span><br><span class="line">    <span class="function">override def <span class="title">deserialize</span><span class="params">(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]])</span>: <span class="params">(GenericRecord, GenericRecord)</span></span></span><br><span class="line"><span class="function">    </span>= (keyDeserializer.deserialize(consumerRecord.key()), valueDeserializer.deserialize(consumerRecord.value()))</span><br><span class="line"></span><br><span class="line">    override def getProducedType: TypeInformation[(GenericRecord, GenericRecord)] = createTypeInformation[(GenericRecord, GenericRecord)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过一个工具类将<code>(Record,Record)</code>转化为更好处理的<code>MongoOplogEntry</code>类型</p>
</li>
<li><p>过滤出所有的<code>Insert</code>事件</p>
</li>
<li><p>将<code>Bill</code>的创建时间作为Flink的事件时间</p>
</li>
<li><p>按照<code>toUserId</code>进行分区</p>
</li>
<li><p>设置10秒的时间窗口</p>
</li>
<li><p>在窗口处理函数中对所有<code>MongoOplogEntry</code>对象按照<code>createdAt</code>再排序后依次触发推送服务</p>
</li>
</ul>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>以上介绍了在Mongodb CDC过程中保证数据有序性的两种思路，一种是在Kafka中就保证<code>toUserId</code>相同的数据均有序，这样在消费过程中不需要做窗口计算，只要在需要partition的地方继续使用<code>toUserId</code>进行partition，就能保证数据有序性。这种方案在Kafka connect侧需要做很多的工作，但是能为流式任务带来更好的消费性能，但是由于debezium的局限性，在shard cluster的Mongodb中不能发挥作用。第二种是让Flink消费乱序数据，使用其本身的事件时间窗口计算来重新纠正数据。这种方案会让Flink的效率大打折扣，并且需要保证窗口缓存数据不能超过限制，但是比较通用，使用的时候也需要注意迟到的数据该如何处理。大家可以根据自己的场景来挑选方案，或者如果有更好的方案欢迎一起交流。</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2020/01/20/a16/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2020-03-22 
	</div>
	

	<!-- categories -->
    

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/monogdb/">monogdb<span>1</span></a></li> <li><a href="/tags/kafka/">kafka<span>1</span></a></li> <li><a href="/tags/flink/">flink<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
		   <span class="toc-title">Contents</span>
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Kafka-connect-amp-amp-Kafka"><span class="toc-article-text">Kafka connect &amp;&amp; Kafka</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Flink-streaming"><span class="toc-article-text">Flink streaming</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#最后"><span class="toc-article-text">最后</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2020 TalkWithKeyboard
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
